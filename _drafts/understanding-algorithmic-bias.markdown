---
title: How Technology Reflects Human Bias—And How We Can Fix It
date: 2020-06-25 08:43:00 -04:00
tags:
- Think Piece
- Digital Inclusion
Author: Center for Digital Acceleration Staff
social-image: "/uploads/AI%202%20(2).svg"
thumbnail: "/uploads/AI%202%20(2).svg"
---

With each technological advancement, more decisions in our daily lives are made by algorithms. They increasingly determine what advertisements we see, whether we’re considered for a job, or which books and movies are recommended to us—and how and where law enforcement is deployed in our communities. The ongoing protests to draw attention to, and eradicate, systemic racism in the United States, and in other parts of the world, have made clear something we already knew—humans are biased. Because we are biased, the technology we create is biased, too.

Over the next several months DAI's Center for Digital Acceleration will explore this topic more deeply by trying to answer the following questions in a series of posts. Questions to be explored will include, but not be limited to: (a) How pervasive is algorithmic bias, and which platforms are the worst offenders?; (b) How does algorithmic bias work, and how we can tackle it at a fundamental technological level?; (c) What other responses—policy, digital literacy, etc.—exist to tackle this phenomenon?; (d) What are the effects of algorithmic bias on the broader digital development sector?

<!--more-->

In the meantime, and as we learn more as a team, here is a list of resources we’ve found useful.

## Resources

**Articles**

* “Dissecting racial bias in an algorithm used to manage the health of populations,” *Science Magazine,* Ziad Obermeyer, Brian Powers, Christine Vogeli Sendhil Mullainathan,. It can be read, [here](https://science.sciencemag.org/content/366/6464/447).
* “Facial recognition fails on race, government study says,” *BBC.* It can be read, [here](https://www.bbc.com/news/technology-50865437).
* “Biased policing is made worse by errors in pre-crime algorithms,” *New Scientist,* Matt Reynolds. It can be read, [here](https://www.newscientist.com/article/mg23631464-300-biased-policing-is-made-worse-by-errors-in-pre-crime-algorithms/#ixzz6PchoHZu1).
* “Reclaiming the stories that algorithms tell,” O’Reilly, David G. Grossman. It can be read, [here](https://www.oreilly.com/radar/reclaiming-the-stories-that-algorithms-tell/).
* “Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms”, Brookings Institute, Nicol Turner Lee, Paul Resnick and Genie Barton. It can be read, [here](https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/).
* “I know some algorithms are biased--because I created one,” *Scientific American*, Nicholas T. Young. It can be read, [here](https://blogs.scientificamerican.com/voices/i-know-some-algorithms-are-biased-because-i-created-one/).
* “Discrimination through optimization: How Facebook's ad delivery can lead to skewed outcomes,” Cornell University, Muhammad Ali, Piotr Sapiezynski, Miranda Bogen, Aleksandra Korolova, Alan Mislove, and Aaron Rieke. It can be read, [here](https://arxiv.org/pdf/1904.02095.pdf).
* “Racial disparities in automated speech recognition,” PNAS, Allison Koenecke, Andrew Nam, Emily Lake, Joe Nudell, Minnie Quartey, Zion Mengesha, Connor Toups, John R. Rickford, Dan Jurafsky, and Sharad Goel. It can be read, [here](https://www.pnas.org/content/117/14/7684).
* “Why racial bias is still inherent in biometric tech”, *Raconteur*, Peter Yeung. It can be read, [here](https://www.raconteur.net/technology/biometrics-ethics-bias).

**Books**

* Charlton Mcllwain, *Black Software: The Internet & Racial Justice, from the AfroNet to Black Lives Matter*
* Safiya Umoja Noble, *Algorithms of Oppression: How Search Engines Reinforce Racism*
* Cathy O’Neil, *Weapons of Math Destruction: How big data increases inequality and threatens democracy*
* Hannah Fry, *Hello World*: *How to be human in the age of the machine*
* Caroline Criado Perez, I*nvisible Women: Exposing data bias in a world designed for men*

**Other Resources**

* As part of the human rights watch film festival, on 12 June you can watch “[Coded Bias](https://www.hrwfilmfestivalstream.org/film/coded-bias/)” which follows the work of women who seek to expose racial and gender bias in technology.
* Reflections from [The Engine Room](https://www.theengineroom.org/tech-bias-people-bias/):
* *Future Tense* frequently publishes articles about algorithmic bias - see [here](https://slate.com/technology/2020/02/algorithmic-bias-people-with-disabilities.html) and [here](https://slate.com/technology/2020/03/ice-lawsuit-hijacked-algorithm.html).
* Data & Society’s [AI On The Ground Initiative](https://datasociety.net/research/ai-on-the-ground/)
* *ProPublica’s*  [Machine Bias](https://www.propublica.org/series/machine-bias) research
* University of Pennsylvania [Toolkit for Centering Racial Equity Throughout Data Integration](https://www.aisp.upenn.edu/equity-toolkit/)
* [Research](https://www.ajlunited.org/library/research) from the Algorithmic Justice League
* [Insights](https://cdt.org/insights/?keyword=Algorithmic\+bias&area-of-focus%5B%5D=ai-machine-learning#results) from the Center for Democracy and Technology
* Report by [AI Now Institute](https://ainowinstitute.org/reports.html) housed at NYU