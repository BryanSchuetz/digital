---
title: Algorithms in Health & Development
date: 2020-07-22 14:13:00 -04:00
tags:
- Algorithmic Bias
- Digital Health
- Digital Inclusion
Author: Gratiana Fu
social-image: "/uploads/icons.PNG"
thumbnail: "/uploads/icons.PNG"
---

In the age of [Big Tech](https://en.wikipedia.org/wiki/Big_Tech) and social media, the term algorithm gets thrown around a lot to describe the black box that determines what people see on social media or the suggestions that they receive on Netflix. But what exactly is an algorithm? And, can an algorithm be biased?

<!--more-->

Algorithms are simple. By definition, an algorithm is a set of rules used to achieve a specific objective. These rules can be implemented in different ways – not just by computers, which is a common misconception. Let's take a non-computer example, waking up and getting ready for work. For me, that means going through a routine to  fully wake up every day. As part of that routine, there are many things that I must do to feel awake. They are:

1. Wake up
2. Take a shower
3. Drink some coffee

These three steps I take, and the order in which I do them in, are an algorithm.

![algorithms.PNG](/uploads/algorithms.PNG)`When you Google search "algorithm" these are some of the images that come up, furthering the common misconception that understanding an algorithm is complex.`

Now that we understand what an algorithm is, let's explore whether an algorithm can be biased. What do I mean by algorithmic bias? Algorithmic bias refers to the errors that appear in algorithms that lead to unfair outcomes. To understand what this looks like in practice, I want you to think of the example I described above-- the steps I take to fully wake up.  If I were ask three other people what steps they take to get ready in the morning, their answers would likely be a little bit different than mine and each others. The figure below shows what steps each of these three people might take.

![icons.PNG](/uploads/icons.PNG)

The first person takes the following steps to fully wake up.

1. Wakes up
2. Exercises
3. Takes a shower
4. Drinks coffee

The second person takes the following steps to fully wake up.

1. Wakes up
2. Drinks coffee
3. Brushes their teeth
4. Eats Breakfast

The third person takes the following steps to fully wake up.

1. Wakes up
2. Brushes their teeth
3. Drinks coffee
4. Eats Breakfast

In other words each of them has their own unique steps they take to wake up. Now imagine, if I were to create a set of rules (an algorithm) to explain or predict the steps that every person in the entire world takes to get ready in the morning based on these three people?  The algorithm I would build  would be a very bad algorithm, because it would fail to account for people who do not drink coffee, or others who have a different routine.

Of course, this example is an vast oversimplification of algorithmic bias but it shows how each individual perspective and life experience influences the rules or steps they take to complete the same task or achieve the same objective.

The rules an algorithm follows matter a lot more when the stakes are higher. Algorithmic bias shows up everywhere, from [search engines](https://time.com/5318918/search-results-engine-google-bias-trusted-sources/) to [hiring practices](https://resources.workable.com/stories-and-insights/unconscious-bias-in-recruitment) to [healthcare delivery](https://news.uchicago.edu/story/health-care-prediction-algorithm-biased-against-black-patients-study-finds).

There are a number of mobile applications that help to detect skin cancer by analyzing a picture of a mole or discoloration on a person's skin and then predicting whether that mole or discoloration could be cancerous. It’s a useful tool that provides a way to pre-screen people before going to see a specialist but [multiple studies](https://www.theatlantic.com/health/archive/2018/08/machine-learning-dermatology-skin-color/567619/) found that these applications are more accurate at detecting skin cancer on people with white skin versus black and brown skin. Meaning that if this tool were to be relied upon solely as a determinant of whether a person should go see a skin cancer specialist, it would likely misdiagnose people with black or brown skin, therefore risking their health.

Efforts to address this type of bias continue. Last month, for instance, a story came out about Malone Mukwende, a second-year medical student at St. George’s in London who created a booklet on clinical care guidelines for people with black and brown skin. His goal was  to bring awareness of how symptoms of certain diseases present differently on darker skin, something that had not been addressed in his formal medical education.

![ou_200710_bame_handbook_kawasaki_disease_malone_mukwende_575x600.jpg](/uploads/ou_200710_bame_handbook_kawasaki_disease_malone_mukwende_575x600.jpg)`An image of how Kawasaki disease, a potential side effect of COVID-19 in children, manifests itself differently on lighter and darker skin.`

As technology continues to become an integral part of peoples lives, there will only be more opportunities to use advanced algorithms in healthcare and the international development space. However, for these tools to truly be effective, we have to learn from the mistakes made, and rectify and account for algorithmic bias to avoid excluding any individual or group. Applications of algorithms at a mass scale, through AI and machine learning techniques in many of the countries where DAI works are still relatively nascent so it's up to us, as responsible development professionals, to imbue these lessons into the tools we design or promote in countries where we work. 

In the meantime, gere are some suggestions you can take to mitigate algorithmic bias:

1. **Diversify your staff**: Algorithms are only as inclusive the team that creates them are.  The lack of people of color, especially Black people, in technology and in development is so much more apparent in the biased algorithms that we use. 
2. **Use appropriate data and information to create your algorithms**: Bad data leads to bad algorithms. Like the example above, we cannot create algorithms with a small subset of the population and expect it to work for the general population. In a similar vein, we cannot simply export a solution that has been developed in the Global North and expect it to work in the Global South.
3. **Question if automation is necessary and/or appropriate**: Algorithms that are executed by a computer are much more rigid than algorithms executed by a human being. As humans, we are able to make split-second decisions - decisions that in a healthcare setting could save someone's life - which computers are unable to do. Ask yourself if it is necessary to automate an algorithm - in many cases, it's neither necessary nor appropriate.
4. **Raise your individual awareness of algorithms**: This suggestion goes beyond just development and health. Big tech companies like Facebook and Google are using algorithms every day to control what we see and what we don't see.  Take a closer look - try to identify the different algorithms that are operating around you and how they might be biased.

Interested in learning more? Here are some additional resources in addition to those listed on our previous [post](https://dai-global-digital.com/understanding-algorithmic-bias.html):

1. [Algorithms of Oppression](https://nyupress.org/9781479837243/algorithms-of-oppression/) by Dr. Safiya Noble
2. [Automating Inequality](https://us.macmillan.com/books/9781250074317) by Virginia Eubanks
3. [Data Feminism](https://data-feminism.mitpress.mit.edu/) by Catherine D’Ignazio and Lauren F. Klein