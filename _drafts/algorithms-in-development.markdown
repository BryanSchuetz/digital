---
title: Algorithms in Health & Development
date: 2020-07-22 14:13:00 -04:00
categories:
- Information
tags:
- Algorithmic Bias
- Digital Health
Author: Gratiana Fu
---

Algorithms. In the age of Big Tech and social media, the term algorithm gets thrown around a lot to describe the black box that determines what people see on social media or the suggestions that they receive on Netflix. But what exactly is an algorithm?

<!--more-->

Algorithms are simple. By definition, an algorithm is a set of rules used to achieve a specific objective. These rules can be implemented in different ways – not just by computers. For example, for me to fully wake up every day, there are many things that I must do:

1. Wake up

2. Take a shower

3. Drink some coffee

These three steps I take and the order I do them in are an algorithm.

![algorithms.PNG](/uploads/algorithms.PNG)

Let's move onto algorithmic bias. Algorithmic bias refers to the errors that appear in algorithms that lead to unfair outcomes. What does that look like in practice? Let’s go back to coffee and waking up. If we ask three other people what steps they take to get ready in the morning, their answers are a little bit different. The first person exercises, showers, and drinks coffee. The second person drinks coffee, brushes their teeth, and eats breakfast while the third person brushes their teeth before drinking coffee, and eats breakfast.

![icons.PNG](/uploads/icons.PNG)

If we were to create a set of rules to explain or predict the steps that every person in the entire world take to get ready in the morning based on these three people, it would be a very bad algorithm. It would fail for peole who don't drink coffee. It would fail for those who have

This is an vast oversimplification of algorithmic bias but it shows how each individual perspective and life experience influences the rules and methods that we choose to complete the same objective. The rules matter a lot more when the stakes are higher – algorithmic bias shows up everywhere, from [search engines](https://time.com/5318918/search-results-engine-google-bias-trusted-sources/) to [hiring practices](https://resources.workable.com/stories-and-insights/unconscious-bias-in-recruitment) to [healthcare delivery](https://news.uchicago.edu/story/health-care-prediction-algorithm-biased-against-black-patients-study-finds).

There are a number of mobile applications that help to detect skin cancer by analyzing a picture of a mole or discoloration on a person's skin and then predicting whether the  you whether it could be cancerous or not. It’s a useful tool that provides a way to pre-screen people before going to see a specialist. But [multiple studies](https://www.theatlantic.com/health/archive/2018/08/machine-learning-dermatology-skin-color/567619/) found that these applications are more accurate at detecting skin cancer on people with white skin versus black and brown skin.

A story came out last month about Malone Mukwende, a second-year medical student at St. George’s in London who created a booklet on clinical care guidelines for people with black and brown skin – he wanted to bring awareness of how symptoms of certain diseases present differently on darker skin, something that had not been addressed in his formal medical education.

![ou_200710_bame_handbook_kawasaki_disease_malone_mukwende_575x600.jpg](/uploads/ou_200710_bame_handbook_kawasaki_disease_malone_mukwende_575x600.jpg)

There will only be more opportunities to use advanced algorithms in healthcare and the international development space like the one behind the skin cancer application. However, we have to learn from the mistakes that have been made in to ensure that the tools we use are not favoring or excluding any individual or group. Applications of AI and machine learning techniques in many of the countries that our team works in are still relatively nascent so it's up to us, as responsible development professionals, to use these tools properly and ethically.

Here are some suggestions you can take to mitigate algorithmic bias:

1\. **Diversify your staff**: Algorithms are only as inclusive the team that creates them are.  The lack of people of color, especially Black people, in technology and in development is so much more apparent in the biased algorithms that we use.

2\. **Use appropriate data and information to create your algorithms**: Bad data leads to bad algorithms. Like the example above, we cannot create algorithms with a small subset of the population and expect it to work for the general population. In a similar vein, we cannot simply export a solution that has been developed in the Global North and expect it to work in the Global South.

3\. **Question if automation is necessary and/or appropriate**: Algorithms that are executed by a computer are much more rigid than algorithms executed by a human being. As humans, we are able to make split-second decisions - decisions that in a healthcare setting could save someone's life - which computers are unable to do. Ask yourself if it is necessary to automate an algorithm - in many cases, it's neither necessary or appropriate.

4\. **Raise your individual awareness of algorithms**: This suggestion goes beyond just development and health. Big tech companies like Facebook and Google are using algorithms every day to control what we see and what we don't see.  Take a closer look - try to identify the different algorithms that are operating around you and how they might be biased.

Interested in learning more? Here are some additional resources:

1. [Algorithms of Oppression](https://nyupress.org/9781479837243/algorithms-of-oppression/) by Dr. Safiya Noble

2. [Automating Inequality](https://us.macmillan.com/books/9781250074317) by Virginia Eubanks

3. [Data Feminism](https://data-feminism.mitpress.mit.edu/) by Catherine D’Ignazio and Lauren F. Klein