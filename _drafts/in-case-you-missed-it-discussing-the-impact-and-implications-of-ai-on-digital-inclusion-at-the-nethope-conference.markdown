---
title: In Case You Missed It—Discussing the Impact and Implications of AI on Digital
  Inclusion at the NetHope Conference
date: 2022-11-08 15:27:00 -05:00
tags:
- AI
Author: Gratiana Fu
social-image: "/uploads/arno-senoner-42t-DKecmPk-unsplash%20(1).jpg"
thumbnail: "/uploads/arno-senoner-42t-DKecmPk-unsplash%20(1).jpg"
---

Last month, DAI’s Center for Digital Acceleration hosted a panel discussion during the NetHope Global Summit featuring artificial intelligence experts representing the public, private, and civil society sectors to discuss practical uses for facial recognition technology (FRT) and their impact on digital inclusion. The panelists presented specific case studies where they have seen their work intersect with AI and facial recognition technologies and digital inclusion, providing lessons learned for attendees to apply. 

<!--more--> 

## Meet Our Speakers

![Jai-Vipra-Picture-JNC.jpg](/uploads/Jai-Vipra-Picture-JNC.jpg){:.float-left}*Our first speaker, Jai Vipra, is a Senior Resident Fellow at the Centre for Applied Law and Technology Research at the [Vidhi Legal Center](https://vidhilegalpolicy.in/). Her work focuses on the economics of digital platforms and the regulatory implications surrounding emerging technologies. 

In her presentation, Jai shared her research on the use of FRTs, specifically closed-circuit television (CCTV) camera networks, for policing in Delhi, India. The study mapped police stations and CCTV camera networks alongside population demographics. She found that police stations in Delhi are spread unevenly throughout the city with some areas of Old Delhi being more policed than others. Overpoliced and over-surveilled areas had a large proportion of Muslim residents, demonstrating that the most surveilled communities and/or those most vulnerable to the negative impacts of FRTs in Delhi were Muslim residents. You can read Jai’s full research report here.

![maddie.png](/uploads/maddie.png){:.float-left}* Our second speaker was Madeleine Stone, a Legal and Policy Officer with [Big Brother Watch](https://bigbrotherwatch.org.uk/) in the United Kingdom (UK). Madeleine’s work has largely focused on the impact of emergency COVID-19 regulations on civil liberties. She has worked with a range of organizations that promote freedom of expression in the UK and globally, including English PEN, Index on Censorship, and Lawyers Without Borders. 

Madeleine presented on the present use of FRT in public spaces in the UK, including by schools, police, and shops. She spoke about the campaigns that [Big Brother Watch](https://bigbrotherwatch.org.uk/) manages, most of which aim to improve public awareness and knowledge of FRT operating in public space. She further discussed that although proponents of FRTs claim that the technologies can be used to identify missing children or those who are being trafficked, there is very little evidence to suggest that the technologies are used for anything more than policing.

![maddie.png](/uploads/maddie.png){:.float-left}*Our final speaker was Danilo Krivokapić, the Director of the [SHARE Foundation](https://www.sharefoundation.info/en/), a Digital Rights organization based in Belgrade, Serbia. A lawyer by training and education, his fields of work and expertise include data protection, the impact of data-driven business models on privacy, legal standards for information security, and cybercrime. He is also the founder of the “[Thousands of Cameras](https://privacyinternational.org/case-study/3967/thousands-cameras-citizen-response-mass-biometric-surveillance)” initiative whereby individuals and organizations advocate the responsible use of surveillance technology.

Like Jai and Madeleine, Danilo used his work as a case study and spoke about the presence of FRT in Serbian public spaces. He additionally discussed the SHARE Foundation’s efforts to gather and disseminate resources to safeguard citizens’ online rights. His presentation focused on the role of law and governance in their work and the resulting challenges that he and his organization face in organizing to regulate and mitigate the negative impacts of FRT in Serbia.

## Key Takeaways
Following the presentations and dialogue between panelists, we came to the following conclusions: 
* FRTs in general have low accuracy when it comes to identifying individuals. These technologies are more inaccurate when identifying individuals with marginalized identities.
* There was consensus among presenters that FRTs, even if they were 100% accurate, should be used with extreme caution or not at all given the ethical and privacy concerns they present. 
* In each FRT case study, there were concerns over the lack of regulations guiding FRT implementation and the use of FRT for state surveillance. Such regulations are essential for the ethical use of FRTs. 

While the novelty of FRTs might energize digital development practitioners, this discussion encourages us to take a closer look at the negative impact of emerging technologies and consider whether and how these technologies include a variety of users. In this case, it does not seem that FRTs are incompatible with and digital inclusion.

