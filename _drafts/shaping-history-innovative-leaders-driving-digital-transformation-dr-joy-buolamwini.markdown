---
title: 'Shaping History: Innovative Leaders Driving Digital Transformation  —  Dr.
  Joy Buolamwini'
date: 2022-02-17 08:16:00 -05:00
Author: Inta Plostins
---

Aligned with the Center for Digital Acceleration’s commitment to fostering equity and inclusion within our team and the broader international development sector, Digital @ DAI is launching our new blog series during Black History Month called Shaping History: Innovative Leaders Driving Digital Transformation. The series aims to spotlight racially diverse leaders in the tech space at home and in the countries where we work and discuss their work implications for the international development field. Without further ado, onto our first profile — Dr. Joy Buolamwini.

<!--more-->

## **Dr. Joy Buolamwini – Poet of Code**

Computer scientist and activist Dr. Buolamwini is one of the world’s leading voices examining algorithmic harms and biases, with a particular focus on racial and gender bias in artificial intelligence (AI). As told in the documentary *[Coded Bias](https://www.netflix.com/title/81328723)*, Dr. Buolamwini, who is Black, grew interested in this area when popular facial recognition software did not recognize her face until she *literally* put on a white mask. In part from this experience, her doctoral research at the Massachusetts Institute of Technology (MIT) developed the concept of an “evocative audit” [rooted](https://www.media.mit.edu/events/joy-buolamwini-defense/) in “black feminist epistemology, intersectionality, and the outsider within standpoint.” Unlike traditional algorithmic audits intended to be depersonalized examinations of whether algorithms are actually doing what they are intended to do in a narrow technical sense, [evocative audits](https://ethanzuckerman.com/2021/10/07/hope-and-joy/) are deliberately designed to show the human impact of algorithmic systems on individual people. In an [interview](https://www.getrevue.co/profile/themarkup/issues/is-the-face-the-final-frontier-of-privacy-1000994) with The Markup, Dr. Buolamwini said of evocative audits: “The whole point of doing this is to actually invite you to empathize. What does it mean to face algorithmic harm or to experience machine erasure or denigration in some way? That’s what the evocative audit does. It draws you in to show why these systems matter and how they can impact people in a negative way.”

While working on her doctoral research at MIT, Dr. Buolamwini’s [other work](https://poetofcode.com/research/) launched her onto the national stage. Most notably, her 2018 [paper](https://dam-prod.media.mit.edu/x/2018/02/06/Gender%20Shades%20Intersectional%20Accuracy%20Disparities.pdf) with Dr. Timnit Gebru on racial and gender bias in facial recognition algorithms has been [cited](https://scholar.google.com/scholar?cites=14954608238029559254&as_sdt=40000005&sciodt=0,22&hl=en) over 2400 times in other works, all in less than four years since its initial publication. Stemming from her collective body of work, Dr. Buolamwini has been named to Time’s Time 100 Next, Forbes’s 30 Under 30, and MIT Tech Review’s 35 Under 35, [among others](https://poetofcode.com/about/). As a result of this recognition for her groundbreaking work, she has become a public intellectual in the technology space, regularly highlighting algorithmic harms that will materially impact individual people, especially people of color. Most recently, in January 2022, she wrote [an op-ed for ](https://www.theatlantic.com/ideas/archive/2022/01/irs-should-stop-using-facial-recognition/621386/)*[The Atlantic](https://www.theatlantic.com/ideas/archive/2022/01/irs-should-stop-using-facial-recognition/621386/)* about the Internal Revenue Service’s (IRS) proposed use of facial recognition on its website. Her article launched a broader conversation around the use of facial recognition to access government services in the United States, which \[quickly \](http://Her article launched a broader conversation around the use of facial recognition to access government services in the United States, which quickly led to the IRS distancing itself from its original plans.)led to the IRS distancing itself from its original plans.

In addition to her academic work, Dr. Buolamwini is an artist – specifically a *[poet of code](https://poetofcode.com/art/)*. Her spoken-word piece [“AI, Ain’t I A Woman?”](https://www.youtube.com/watch?v=QxuyfWoVV98) — viewed almost 100,000 times on YouTube — uses Sojourner Truth’s famous question “Ain’t I A Woman?” as a launching point to show how commercially available facial recognition technologies misgender well-known women of color, especially Black women. In her [interview](https://www.getrevue.co/profile/themarkup/issues/is-the-face-the-final-frontier-of-privacy-1000994) with The Markup, she also explicitly identifies this piece as an evocative audit, showing how evocative audits can take a wide variety of forms.

![poet of code.jpg](/uploads/poet%20of%20code.jpg)\
`Photo Credit: Megan Smith (former Chief Technology Officer of the USA), AJL.org.`

## Implications of Dr. Buolamwini's work for international development 

Dr. Buolamwini’s work has multiple potential applications to and implications for the international development sector. A few initial thoughts:

**Be wary of facial recognition and other biometric identification tools.** Even though biometrics are already relatively widely used within the international development sector (especially in the [humanitarian space](https://blogs.icrc.org/law-and-policy/2021/09/02/biometrics-humanitarian-delicate-balance/)), Dr. Buolamwini’s work highlights that biometrics like facial recognition have serious limitations, including in emerging markets. International development is still largely Global North-driven. If facial recognition technology used in the Global South is developed in the Global North using training datasets consisting of primarily white faces, the AI will not work as intended. Even leaving aside the thorny ethical questions of informed consent and data privacy, deploying facial recognition that does not work well on the target population defeats the purpose of using facial recognition at all. For organizations who want to use facial recognition in the Global South, there are two practical options: either deploy training datasets featuring people who are from the country where you are working, or better yet (for all the reasons embodied by Dr. Buolamwini’s work on algorithmic harms and bias), do not use facial recognition systems at all.

**Institutionalize identifying and [fighting](https://dai-global-digital.com/algorithms-in-development.html) algorithmic harms and bias in your digital tools, centering people in digital development. **Dr. Buolamwini’s academic work has primarily, though not exclusively, focused on facial recognition. Her body of work points to the need to identify and mitigate the effects of algorithmic harms and bias in the digital tools that we use in our work. Even though algorithmic bias and algorithmic harms may seem like a high-level, abstract concept, algorithms hold real power, from the [jobs we hold](https://hbr.org/2019/05/all-the-ways-hiring-algorithms-can-introduce-bias) to [where we live](https://www.newamerica.org/oti/blog/huds-new-rule-paves-the-way-for-rampant-algorithmic-discrimination-in-housing-decisions/) to the [money available to us](https://hai.stanford.edu/news/how-flawed-data-aggravates-inequality-credit#:\~:text=It's%20not%20that%20the%20credit,borrowers%20have%20limited%20credit%20histories.). The Global South is not exempt from these dynamics, nor is (especially) the international development sector’s[ work in the Global South](https://www.worldbank.org/en/events/2021/10/24/algorithmic-racial-bias-a-development-challenge). However, Dr. Buolamwini’s work with evocative audits — described in her [PhD thesis abstract](https://www.media.mit.edu/events/joy-buolamwini-defense/) as “an approach to humanizing the negative impacts that can result from algorithmic systems… \[and\] that allow others to bear witness to issues created by algorithmic systems of interest” — reminds us to center individual people and their lived experience when it comes to algorithms. Adapting the idea of an evocative audit for the digital development sector, centering the experiences of individual people in the Global South by examining how they interact with algorithms and how those algorithms affect their lives, could be a significant step forward toward countering algorithmic bias and harms within the international development sector.

*Dr. Buolamwini’s [website](https://www.poetofcode.com/) and her[ Twitter](https://twitter.com/jovialjoy), [Instagram](https://www.instagram.com/joyfulcode/), [LinkedIn](https://www.linkedin.com/in/buolamwini/), and [Medium](https://medium.com/@Joy.Buolamwini) pages can be found here.*