---
title: 'Digital Identity Series Part 3:  Digital Identity and Exclusion'
date: 2019-07-02 10:42:00 -04:00
tags:
- Digital Identity Series
- Digital Inclusion
- Privacy
Author: Chloe Messenger
social-image: "/uploads/Digital%20ID%20small-1-ab2324.jpg"
---

![Digital ID small-1-97efcb.jpg](/uploads/Digital%20ID%20small-1-97efcb.jpg){:.float-left}Any ID - whether state-issued or purpose-made for services such as cash transfers - in some way reflects levels of equality, whether these are inherent in the technology or prevalent in the social ecosystem. For instance, [GSMA research](https://www.gsma.com/mobilefordevelopment/wp-content/uploads/2019/05/Digital-identity-opportunities-for-women-Insights-from-Nigeria-Bangladesh-and-Rwanda-Web.pdf) in Nigeria, Bangladesh, and Rwanda showed that citizens feel that equality in national ID is important as it reflects equality in citizenship status for men and women.  If a biometric-based ID card is created within a refugee camp to facilitate cash transfers, every person living in that camp can rightfully expect to be registered regardless of age, gender or ethnicity.

In this blog we look at a number of ways in which bias can manifest in digital identities in the hope that this kicks off some thinking and a step towards identifying solutions. While reading this, I encourage you to think how can we make sure digital ID systems are inclusive? How can we be thinking about all the different groups that can be excluded?
<!--more-->

![tech1.jpg](/uploads/tech1.jpg)
Some technologies such as biometrics and AI can be inherently technologically biased. Bias in facial recognition technology is [well documented:](https://www.media.mit.edu/articles/facial-recognition-software-is-biased-towards-white-men-researcher-finds/) often with a broad inability to recognise the faces of certain ethnic groups, and more difficulty recognising women than men. Having been designed largely by a tech sector dominated by white men, the resultant technologies struggle to recognise other ethnicities – and particularly women within those - with [alarming inaccuracy](https://www.theverge.com/2019/1/25/18197137/amazon-rekognition-facial-recognition-bias-race-gender).

Fingerprints can be worn by manual labour, rendering them unreadable; age-related issues like cataracts can mean older peoples irises cannot be read. These changes present challenges to those relying on their biometrics to identify them for services such as voting, receipt of cash transfers or food aid. Bias or inaccuracies in biometrics can be overcome by regular biometric taking, checking data at regular intervals to identify gradual changes. However, where biometrics form part of an identity card, regular checks are unlikely to happen among the marginalised/rural/harder for IDPs and refugees who are often mobile. This can create further marginalisation of these groups. Whether it is ethnicity, age or gender, more errors take place among minorities where systems are built for the majority.

![soc3.jpg](/uploads/soc3.jpg)
Technology – however smart – is not exempt from human bias. Often the implementation of technology acts as a mirror image or amplifier of divides and biases in society.

The write-up from a recent [Tech Salon ](http://technologysalon.org/how-we-can-control-our-digital-identities/?utm_source=dlvr.it&utm_medium=twitter)in Washington DC summarises this succinctly: “ID is embedded in your relationships and networks.”  We need to understand what is going on at the individual level and how becoming more identifiable - and the systems by which this takes place - could impact on and be defined by a person’s relationships and networks.

It is no surprise that there is a complex gender narrative associated with identity. A [GSMA study](https://www.gsma.com/mobilefordevelopment/wp-content/uploads/2019/05/Digital-identity-opportunities-for-women-Insights-from-Nigeria-Bangladesh-and-Rwanda-Web.pdf) in Nigeria, Rwanda and Bangladesh found a perception that men need the ID more than women, with ID being seen as something that professionals and business people have.  Indeed, women and girls are often seen to have less need for an ID, as they can rely on their husbands or fathers ID for access to services. Caribou Digital’s [research](https://medium.com/caribou-digital/when-id-works-for-women-initial-findings-from-bangladesh-56898724f792) in Bangladesh found that the job a woman hoped to hold greatly affected her perception of the need for an ID card.

In their paper [Identity at the Margins](https://assets.publishing.service.gov.uk/media/5cecedd6ed915d2475aca8c5/Identity-At-The-Margins-Identification-Systems-for-Refugees.pdf), Caribou Digital also outlines how systems that record beneficiaries at a household rather than individual level can impact household power dynamics.  They cite the case of Uganda, where South Sudanese women who fled before male relatives were registered as the head of the household, giving them the title of official recipient of aid, but bringing violence to them from male heads of household who saw this as a disruption to the patriarchy (War Child research in Bidi Bidi).

Marginalised communities such as migrants are also often at the blunt end of bias when it comes to digital ID. For instance, faith in the robustness and reliability of the biometric system could mean marginalised groups are not trusted when it doesn’t work for them- reinforcing existing inequalities. [GenderIT research ](https://www.genderit.org/feminist-talk/what-lies-behind-fears-digital-identity-experience-huduma-number-kenya)in Kenya found a perception of your “last name betrays you" in terms of tribal politics, so the risks of this being embedded in a digital ID is scary for some in terms of the security of that data and who has access to it.

In the first blog on informed consent we spoke about how cultural beliefs can be a barrier to some biometric data collection. Barriers such as time, being expected to stay near home can also play a part in reinforcing social norms into the ID space.

![burea4.jpg](/uploads/burea4.jpg)
In my mental map of these biases, bureaucratic bias sits somewhere between biased tech and reflecting societal bias. It covers the way in which implementers collect and process the identity data and how this can reflect or exacerbate existing bias.

[Data society ](https://datasociety.net/wp-content/uploads/2019/04/DataSociety_DigitalIdentity.pdf)define it as including the classification of vulnerable communities and the inconsistent collection of migrants’ identity information.  By talking about classification, we refer to cases whereby individuals are classified by political or economic identifiers – for instance someone identified as an economic migrant may be treated completely differently to someone fleeing conflict. The inclusion of such a description as part of ones digital identity can alter the way they are perceived and treated. Additionally, many migrants perceive biometric data collection as inherently connected to government and law enforcement—which many have learned to treat with scepticism, often rightly so.

Practicalities of the bureaucracies associated with registering for a formal identity can also exacerbate inequalities. [GenderIT research](https://www.genderit.org/feminist-talk/what-lies-behind-fears-digital-identity-experience-huduma-number-kenya) in Kenya found that strict deadlines and the extensive travel required mean many women aren’t able to get the ID card.   In a situation where someone feels unsafe travelling into the city, cannot afford to, or do not have permission from their head of household, the time taken to register and collect an identity card or sign up for biometrics may be too much.

![reg3.jpg](/uploads/reg3.jpg)
The Digital ID series blog 2 focused on Digital ID and politics, but I wanted to mention here an example of how regulations can exacerbate inequality.
Digital identity systems that rely on mobile technology are restricted by KYC requirements and biased towards those with ID cards – i.e. citizens. For instance, in Bangladesh, Rohingya refugees lack the required forms of ID to be registered with a digital identity. The BTRC has reportedly banned the sale of SIMs to Rohingya refugees and individuals have been arrested for selling mobile devices and [SIMs to Rohingy](https://www.unhcr.org/innovation/displaced-and-disconnected/)a.   Also in Bangladesh, individuals can register up to 15 sim cards, so many women rely on men to register for them, thus perpetuating gender barriers. This is not an isolated issue: 173 countries are hosts to 19.9 million refugees, yet [according to the GSMA,](https://www.gsma.com/mobilefordevelopment/blog/access-to-mobile-services-and-proof-of-identity-2019-assessing-the-impact-on-digital-and-financial-inclusion/) 75 per cent of these countries legally require people to present an acceptable proof of identity in order to register for a mobile SIM card.

My recent reading on the subject has flagged up these four “buckets” of barriers to inclusion in the digital ID space, but the list is by no means exhaustive. Research being done by thought leaders such as GSMA, Caribou Digital and the Omidyar network are always bringing us a step closer to understanding how we can design inclusive digital identity programmes. Are you working on digital ID? Please share your lessons with us on Twitter @DAIGlobal